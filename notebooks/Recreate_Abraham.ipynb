{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Grading of Prostate Cancer  \n",
    "### Using Convolutional Neural Network and Ordinal Classifier  \n",
    "\n",
    "**Authors:** Abraham, Bejoy, and Nair, Madhu S.  \n",
    "**Python Implementation:** Adapted by Edward Sandoval  \n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "This notebook aims to implement the **C4.5 ordinal classification algorithm** (referred to as J48) described in the cited paper for the **PICAI dataset**, focusing on prostate lesion malignancy classification into three ordinal classes.  \n",
    "\n",
    "---\n",
    "\n",
    "## Malignancy Class Mapping\n",
    "The following class mappings are applied to group ISUP grades into ordinal malignancy levels:  \n",
    "\n",
    "| **ISUP Grades** | **Malignancy Class** |  \n",
    "|------------------|-----------------------|  \n",
    "| 0, 1            | Class 0              |  \n",
    "| 2, 3            | Class 1              |  \n",
    "| 4, 5            | Class 2              |  \n",
    "\n",
    "---\n",
    "\n",
    "## Note\n",
    "To ensure proper execution and compatibility with all dependencies, **it is recommended to run this notebook within the container generated from the `compose_weka.yml` file in the `container` folder**. This ensures the environment aligns with the requirements of the implemented algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/app/')\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "DATA_PATH = '/Datasets/PICAI_64x64_patches/'\n",
    "info = json.load(open(DATA_PATH + '8x64x64-CIspheres.json'))\n",
    "folds_idxs = json.load(open(DATA_PATH + 'picai_patches_splits_5kf.json'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 8, 64, 64)\n",
      "0.0 0.7118942737579346\n"
     ]
    }
   ],
   "source": [
    "test_data = np.load(DATA_PATH+'patches/10144_1000146_000.npy')\n",
    "print(test_data.shape)\n",
    "print(np.min(test_data),np.max(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_label(idx, type='bin1'):\n",
    "    \n",
    "    label = info[idx]['case_ISUP']\n",
    "    if type == 'multi1':\n",
    "        multi1 = {0:0,1:0,2:1,3:1,4:2,5:2}\n",
    "        label = multi1[label]\n",
    "    elif type == 'multi2':\n",
    "        label = label\n",
    "    elif type == 'bin1':\n",
    "        bin1 = {0:0,1:0,2:1,3:1,4:1,5:1}\n",
    "        label = bin1[label]\n",
    "\n",
    "    return label\n",
    "\n",
    "def preprocess_section(patch):\n",
    "    \"\"\"\n",
    "    Receives a patch from dimensions 3,8,64,64 refering to modalities, slices, height and width\n",
    "    Gets the center patch across the slices and resizes it to 224,224\n",
    "    \"\"\"\n",
    "    #Convert to pytorch tensor\n",
    "    patch = torch.tensor(patch[:, 3:6, :, :])\n",
    "    \n",
    "    #Resize image\n",
    "    patch = F.interpolate(patch, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "   \n",
    "    #Convert to float\n",
    "    patch = patch.float()\n",
    "    \n",
    "    return patch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3000])\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "\n",
    "model = timm.create_model(\n",
    "    'vgg16.tv_in1k',\n",
    "    pretrained=True,\n",
    "    features_only=False, #This parameters allows to obtain the last layer or all the previous ones\n",
    ")\n",
    "model = model.eval()\n",
    "\n",
    "def get_embedding(model, input):\n",
    "    \"\"\"\n",
    "    Receives a patch from dimensions 3,8,64,64 refering to modalities, slices, height and width\n",
    "    Gets the center patch across the slices and resizes it to 224,224\n",
    "    \"\"\"\n",
    "    #Preprocess patch\n",
    "    input = preprocess_section(input)\n",
    "    \n",
    "    #Get embeddings\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(input)\n",
    "\n",
    "    #flatten embeddings\n",
    "    embeddings = embeddings.view(1, -1)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "test_embedding = get_embedding(model, test_data)\n",
    "print(test_embedding.shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def get_Xy(idxs, label_type='multi1'):\n",
    "    embs = []\n",
    "    ys = []\n",
    "    for i in tqdm(idxs, desc=\"Processing\"):  # Add tqdm to iterate with a progress bar\n",
    "        img = np.load(DATA_PATH + f'patches/{i}.npy')\n",
    "        y = get_label(i, label_type)\n",
    "        if info[i]['case_ISUP'] != 1:  \n",
    "            emb = get_embedding(model, img)\n",
    "            embs.append(emb), ys.append(y)\n",
    "\n",
    "    # Convert the list of torch tensors to a NumPy array\n",
    "    embs = torch.cat(embs, dim=0).numpy()  # Concatenate along batch dimension and convert to NumPy\n",
    "    ys = np.array(ys)  # Convert list of integers to NumPy array\n",
    "\n",
    "    print(f\"   Final length of {ys.shape[0]}\")\n",
    "\n",
    "    return embs, ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx_fold(fold):\n",
    "    key_train = f'train_fold_{fold}'\n",
    "    key_val = f'val_fold_{fold}'\n",
    "\n",
    "    return folds_idxs[key_train], folds_idxs[key_val]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score\n",
    "\n",
    "def get_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes precision, recall, F1-score (macro average), and weighted Cohen's kappa.\n",
    "\n",
    "    Args:\n",
    "        y_true (list or np.array): Ground truth labels.\n",
    "        y_pred (list or np.array): Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with precision, recall, F1-score, and weighted kappa.\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"precision_macro\": precision_score(y_true, y_pred, average='macro'),\n",
    "        \"recall_macro\": recall_score(y_true, y_pred, average='macro'),\n",
    "        \"f1_macro\": f1_score(y_true, y_pred, average='macro'),\n",
    "        \"weighted_kappa\": cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "    }\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from weka.core.dataset import create_instances_from_matrices\n",
    "from weka.core.converters import Loader\n",
    "from weka.classifiers import Classifier\n",
    "import weka.core.jvm \n",
    "from weka.filters import Filter\n",
    "\n",
    "\n",
    "def create_dataset_weka(x,y):\n",
    "    dataset = create_instances_from_matrices(x, y, col_y='class')\n",
    "\n",
    "    # Convert the class attribute to nominal\n",
    "    dataset.class_is_last()  # Ensure the class is the last column\n",
    "    numeric_to_nominal = Filter(classname=\"weka.filters.unsupervised.attribute.NumericToNominal\", options=[\"-R\", \"last\"])\n",
    "    numeric_to_nominal.inputformat(dataset)\n",
    "    nominal_dataset = numeric_to_nominal.filter(dataset)\n",
    "\n",
    "    return nominal_dataset\n",
    "\n",
    "\n",
    "def predict_weka(cls, nominal_dataset):\n",
    "\n",
    "    preds = []\n",
    "    # Make predictions\n",
    "    for index, inst in enumerate(nominal_dataset):\n",
    "        pred = cls.classify_instance(inst)\n",
    "        #dist = cls.distribution_for_instance(inst)\n",
    "        #print(f\"{index + 1}: label index={pred}, class distribution={dist}\")\n",
    "        preds.append(pred)\n",
    "    return np.array(preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:weka.core.jvm:Adding bundled jars\n",
      "DEBUG:weka.core.jvm:Classpath=['/usr/local/lib/python3.10/dist-packages/javabridge/jars/rhino-1.7R4.jar', '/usr/local/lib/python3.10/dist-packages/javabridge/jars/runnablequeue.jar', '/usr/local/lib/python3.10/dist-packages/javabridge/jars/cpython.jar', '/usr/local/lib/python3.10/dist-packages/weka/lib/weka.jar', '/usr/local/lib/python3.10/dist-packages/weka/lib/python-weka-wrapper.jar', '/usr/local/lib/python3.10/dist-packages/weka/lib/core.jar', '/usr/local/lib/python3.10/dist-packages/weka/lib/mtj.jar', '/usr/local/lib/python3.10/dist-packages/weka/lib/arpack_combined.jar']\n",
      "DEBUG:weka.core.jvm:MaxHeapSize=default\n",
      "DEBUG:weka.core.jvm:Package support disabled\n",
      "Nov 30, 2024 7:21:31 PM com.github.fommil.netlib.ARPACK <clinit>\n",
      "WARNING: Failed to load implementation from: com.github.fommil.netlib.NativeSystemARPACK\n",
      "Nov 30, 2024 7:21:31 PM com.github.fommil.netlib.ARPACK <clinit>\n",
      "WARNING: Failed to load implementation from: com.github.fommil.netlib.NativeRefARPACK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current FOLD: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/873 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  38%|███▊      | 328/873 [01:54<03:09,  2.88it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "from weka.core.dataset import create_instances_from_matrices\n",
    "from weka.core.converters import Loader\n",
    "from weka.classifiers import Classifier\n",
    "import weka.core.jvm as jvm\n",
    "from weka.filters import Filter\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "jvm.start()\n",
    "\n",
    "train_metrics_list = []\n",
    "val_metrics_list = []\n",
    "for f in range(5):\n",
    "    print(f\"Current FOLD: {f+1}\")\n",
    "    idxs_train, idxs_val = get_idx_fold(f)\n",
    "\n",
    "    # Load data\n",
    "    X_train, y_train = get_Xy(idxs_train, label_type='multi1')\n",
    "    X_val, y_val = get_Xy(idxs_val, label_type='multi1')\n",
    "\n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=0.85)\n",
    "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "    X_val_pca = pca.transform(X_val_scaled)\n",
    "    print(f\"    PCA retained {X_train_pca.shape[1]} components.\")\n",
    "\n",
    "    # Train and evaluate C4.5 classifier\n",
    "    # Convert X_train, y_train to a pd dataframe as well as X_val,y_val\n",
    "    dataset_train = create_dataset_weka(X_train_pca,y_train)\n",
    "    dataset_val = create_dataset_weka(X_val_pca,y_val)\n",
    "\n",
    "    cls = Classifier(classname=\"weka.classifiers.trees.J48\", options=[\"-C\", \"0.3\"])\n",
    "    cls.build_classifier(dataset_train)\n",
    "\n",
    "    y_pred_train = predict_weka(cls, dataset_train)\n",
    "    y_pred_val = predict_weka(cls, dataset_val)\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics_train = get_metrics(y_train, y_pred_train)\n",
    "    metrics_val = get_metrics(y_val, y_pred_val)\n",
    "\n",
    "    print(f\"    Train metrics ----> {metrics_train}\")\n",
    "    print(f\"    Valid metrics ----> {metrics_val}\")\n",
    "\n",
    "    # Collect metrics\n",
    "    train_metrics_list.append(metrics_train)\n",
    "    val_metrics_list.append(metrics_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_metrics_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/app/notebooks/Recreate_Abraham.ipynb Cell 10\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6a3438222c2273657474696e6773223a7b22636f6e74657874223a22726f6f746c657373227d7d@ssh-remote%2Bworkstation/app/notebooks/Recreate_Abraham.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         mean_std[key] \u001b[39m=\u001b[39m (np\u001b[39m.\u001b[39mmean(values), np\u001b[39m.\u001b[39mstd(values))\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6a3438222c2273657474696e6773223a7b22636f6e74657874223a22726f6f746c657373227d7d@ssh-remote%2Bworkstation/app/notebooks/Recreate_Abraham.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m mean_std\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6a3438222c2273657474696e6773223a7b22636f6e74657874223a22726f6f746c657373227d7d@ssh-remote%2Bworkstation/app/notebooks/Recreate_Abraham.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m train_metrics_summary \u001b[39m=\u001b[39m compute_mean_std(train_metrics_list)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6a3438222c2273657474696e6773223a7b22636f6e74657874223a22726f6f746c657373227d7d@ssh-remote%2Bworkstation/app/notebooks/Recreate_Abraham.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m val_metrics_summary \u001b[39m=\u001b[39m compute_mean_std(val_metrics_list)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6a3438222c2273657474696e6773223a7b22636f6e74657874223a22726f6f746c657373227d7d@ssh-remote%2Bworkstation/app/notebooks/Recreate_Abraham.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mSummary of Metrics Across Folds:\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_metrics_list' is not defined"
     ]
    }
   ],
   "source": [
    "# Aggregate metrics\n",
    "def compute_mean_std(metrics_list):\n",
    "    mean_std = {}\n",
    "    for key in metrics_list[0]:\n",
    "        values = [metrics[key] for metrics in metrics_list]\n",
    "        mean_std[key] = (np.mean(values), np.std(values))\n",
    "    return mean_std\n",
    "\n",
    "train_metrics_summary = compute_mean_std(train_metrics_list)\n",
    "val_metrics_summary = compute_mean_std(val_metrics_list)\n",
    "\n",
    "print(\"\\nSummary of Metrics Across Folds:\")\n",
    "print(\"Train Metrics (Mean ± Std):\")\n",
    "for key, (mean, std) in train_metrics_summary.items():\n",
    "    print(f\"  {key}: {mean:.4f} ± {std:.4f}\")\n",
    "\n",
    "print(\"Validation Metrics (Mean ± Std):\")\n",
    "for key, (mean, std) in val_metrics_summary.items():\n",
    "    print(f\"  {key}: {mean:.4f} ± {std:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
